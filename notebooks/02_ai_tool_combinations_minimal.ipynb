{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# AI Tool Combinations Analysis: Code Generation & Review Patterns\n",
    "\n",
    "**Research Question:** What combinations of AI tools for code generation and review are prevalent?\n",
    "\n",
    "## Objectives:\n",
    "1. Identify which AI agents generate code vs which ones review code\n",
    "2. Map the most common AI generator ‚Üí AI reviewer combinations\n",
    "3. Analyze success rates of different tool combinations\n",
    "4. Understand workflow patterns in multi-AI environments\n",
    "\n",
    "## Dataset: AIDev - AI Teammates in Software Engineering\n",
    "- 456,535 AI-generated Pull Requests\n",
    "- 5 Major AI Agents: OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code\n",
    "- Complete review histories with human and bot reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Environment Setup and Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Configure display and plotting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('üîß Environment Setup Complete!')\n",
    "print(f'üìö Libraries loaded for AI tool combination analysis')\n",
    "print(f'‚è∞ Analysis started at: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Try these installation methods in order until one works\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"üîß Installing Plotly with multiple methods...\\n\")\n",
    "\n",
    "# Method 1: Standard installation in current kernel\n",
    "print(\"Method 1: Standard installation\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"plotly\"])\n",
    "    print(\"‚úÖ Method 1 successful\")\n",
    "except:\n",
    "    print(\"‚ùå Method 1 failed\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# Method 2: Force reinstall\n",
    "print(\"Method 2: Force reinstall\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"plotly\"])\n",
    "    print(\"‚úÖ Method 2 successful\")\n",
    "except:\n",
    "    print(\"‚ùå Method 2 failed\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "\n",
    "# Method 3: User installation\n",
    "print(\"Method 3: User installation\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"plotly\"])\n",
    "    print(\"‚úÖ Method 3 successful\")\n",
    "except:\n",
    "    print(\"‚ùå Method 3 failed\")\n",
    "\n",
    "print(\"\\nüîÑ NOW RESTART YOUR KERNEL!\")\n",
    "print(\"   Kernel ‚Üí Restart ‚Üí Run All\")\n",
    "\n",
    "# Test import\n",
    "print(\"\\nüß™ Testing import...\")\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    print(\"‚úÖ SUCCESS: Plotly imported!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Still failed: {e}\")\n",
    "    print(\"RESTART YOUR KERNEL and try importing again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install PyArrow for parquet support\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow\"])\n",
    "print(\"‚úÖ PyArrow installed!\")\n",
    "\n",
    "# Also install datasets library for HuggingFace\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\"])\n",
    "print(\"‚úÖ Datasets library installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install fastparquet properly\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fastparquet\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"])\n",
    "\n",
    "print(\"‚úÖ Packages installed - now try the loading strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Strategy 1: fastparquet\n",
    "try:\n",
    "    pull_requests_df = pd.read_parquet(\n",
    "        \"hf://datasets/hao-li/AIDev/all_pull_request.parquet\", \n",
    "        engine=\"fastparquet\"\n",
    "    )\n",
    "    print(f\"‚úÖ SUCCESS! Shape: {pull_requests_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed: {e}\")\n",
    "    # Try Strategy 2: Direct download\n",
    "    import requests\n",
    "    import io\n",
    "    \n",
    "    url = \"https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_pull_request.parquet\"\n",
    "    response = requests.get(url)\n",
    "    pull_requests_df = pd.read_parquet(io.BytesIO(response.content))\n",
    "    print(f\"‚úÖ Direct download SUCCESS! Shape: {pull_requests_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Try loading the AIDev dataset\n",
    "try:\n",
    "    pull_requests_df = pd.read_parquet(\n",
    "        \"hf://datasets/hao-li/AIDev/all_pull_request.parquet\", \n",
    "        engine=\"fastparquet\"\n",
    "    )\n",
    "    print(f\"‚úÖ SUCCESS! Shape: {pull_requests_df.shape}\")\n",
    "    print(f\"Columns: {list(pull_requests_df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(pull_requests_df.head(2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå fastparquet failed: {e}\")\n",
    "    print(\"Trying direct download...\")\n",
    "    \n",
    "    import requests\n",
    "    import io\n",
    "    \n",
    "    url = \"https://huggingface.co/datasets/hao-li/AIDev/resolve/main/all_pull_request.parquet\"\n",
    "    response = requests.get(url)\n",
    "    pull_requests_df = pd.read_parquet(io.BytesIO(response.content))\n",
    "    print(f\"‚úÖ Direct download SUCCESS! Shape: {pull_requests_df.shape}\")\n",
    "    print(f\"Columns: {list(pull_requests_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Explore the real AI agents\n",
    "ai_agents = pull_requests_df[\"agent\"].value_counts()\n",
    "print(\"ü§ñ AI AGENTS IN REAL DATASET:\")\n",
    "print(ai_agents.head(10))\n",
    "\n",
    "print(f\"\\nTotal PRs with AI agents: {pull_requests_df['agent'].notna().sum():,}\")\n",
    "print(f\"Percentage of AI-assisted PRs: {pull_requests_df['agent'].notna().mean()*100:.1f}%\")\n",
    "\n",
    "# Show sample PRs for each AI tool\n",
    "for agent in ai_agents.head(3).index:\n",
    "    sample_prs = pull_requests_df[pull_requests_df[\"agent\"] == agent][\"title\"].head(2)\n",
    "    print(f\"\\n{agent} examples:\")\n",
    "    for title in sample_prs:\n",
    "        print(f\"  - {title[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# LOAD PR REVIEW DATA FOR AI COMBINATION ANALYSIS\n",
    "print('üìÇ LOADING PR REVIEW DATA')\n",
    "print('=' * 35)\n",
    "\n",
    "# Try to load review data using the same successful method\n",
    "review_tables = {}\n",
    "\n",
    "# Common review table names in research datasets\n",
    "possible_review_tables = [\n",
    "    'all_pr_reviews',\n",
    "    'pr_reviews', \n",
    "    'all_pr_review_comments',\n",
    "    'pr_review_comments',\n",
    "    'all_reviews',\n",
    "    'reviews'\n",
    "]\n",
    "\n",
    "print('üîÑ Attempting to load review tables...')\n",
    "\n",
    "for table_name in possible_review_tables:\n",
    "    try:\n",
    "        print(f'  üîç Trying {table_name}...')\n",
    "        \n",
    "        # Use the same fastparquet method that worked for pull requests\n",
    "        review_df = pd.read_parquet(\n",
    "            f\"hf://datasets/hao-li/AIDev/{table_name}.parquet\", \n",
    "            engine=\"fastparquet\"\n",
    "        )\n",
    "        \n",
    "        review_tables[table_name] = review_df\n",
    "        print(f'  ‚úÖ {table_name}: {review_df.shape[0]:,} rows, {review_df.shape[1]} columns')\n",
    "        \n",
    "        # Show columns for first successful table\n",
    "        if len(review_tables) == 1:\n",
    "            print(f'      Columns: {list(review_df.columns)}')\n",
    "            print(f'      Sample data:')\n",
    "            print(f'      {review_df.head(2)}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'  ‚ùå {table_name}: {str(e)[:50]}...')\n",
    "\n",
    "print(f'\\nüìä REVIEW DATA LOADING RESULTS:')\n",
    "print(f'   ‚Ä¢ Successfully loaded {len(review_tables)} table(s)')\n",
    "\n",
    "if review_tables:\n",
    "    # Analyze the loaded review data\n",
    "    print(f'\\nüîç REVIEW DATA STRUCTURE ANALYSIS:')\n",
    "    print('=' * 35)\n",
    "    \n",
    "    for table_name, df in review_tables.items():\n",
    "        print(f'\\nüìã {table_name.upper()}:')\n",
    "        print(f'   ‚Ä¢ Shape: {df.shape}')\n",
    "        print(f'   ‚Ä¢ Columns: {list(df.columns)}')\n",
    "        \n",
    "        # Look for key columns we need for analysis\n",
    "        key_columns = {\n",
    "            'pr_linking': [col for col in df.columns if any(keyword in col.lower() for keyword in ['pr', 'pull', 'request'])],\n",
    "            'reviewer_info': [col for col in df.columns if any(keyword in col.lower() for keyword in ['user', 'login', 'author', 'reviewer'])],\n",
    "            'review_outcome': [col for col in df.columns if any(keyword in col.lower() for keyword in ['state', 'status', 'decision', 'approved'])],\n",
    "            'timing': [col for col in df.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'created', 'submitted'])]\n",
    "        }\n",
    "        \n",
    "        print(f'   üîó PR Linking columns: {key_columns[\"pr_linking\"]}')\n",
    "        print(f'   üë§ Reviewer columns: {key_columns[\"reviewer_info\"]}')\n",
    "        print(f'   ‚úÖ Review outcome columns: {key_columns[\"review_outcome\"]}')\n",
    "        print(f'   ‚è∞ Timing columns: {key_columns[\"timing\"]}')\n",
    "        \n",
    "        # Check for bot/AI reviewers\n",
    "        reviewer_columns = key_columns[\"reviewer_info\"]\n",
    "        if reviewer_columns:\n",
    "            reviewer_col = reviewer_columns[0]  # Use first reviewer column\n",
    "            print(f'\\nü§ñ CHECKING FOR AI/BOT REVIEWERS in {reviewer_col}:')\n",
    "            \n",
    "            # Sample unique reviewers\n",
    "            unique_reviewers = df[reviewer_col].dropna().unique()\n",
    "            \n",
    "            # Look for bot patterns\n",
    "            bot_reviewers = [r for r in unique_reviewers if isinstance(r, str) and 'bot' in r.lower()]\n",
    "            ai_patterns = [r for r in unique_reviewers if isinstance(r, str) and any(pattern in r.lower() for pattern in ['copilot', 'ai', 'claude', 'gpt', 'codex'])]\n",
    "            \n",
    "            print(f'   ü§ñ Bot reviewers found: {len(bot_reviewers)}')\n",
    "            if bot_reviewers:\n",
    "                print(f'      Sample bots: {bot_reviewers[:5]}')\n",
    "            \n",
    "            print(f'   üß† AI-pattern reviewers found: {len(ai_patterns)}')\n",
    "            if ai_patterns:\n",
    "                print(f'      Sample AI reviewers: {ai_patterns[:5]}')\n",
    "            \n",
    "            print(f'   üë• Total unique reviewers: {len(unique_reviewers):,}')\n",
    "            print(f'   üìä Sample reviewers: {list(unique_reviewers[:10])}')\n",
    "\n",
    "    print(f'\\nüéØ NEXT STEPS:')\n",
    "    print('1. Link PR creation data (with AI agents) to review data')\n",
    "    print('2. Identify AI tool (creator) + reviewer combinations') \n",
    "    print('3. Analyze approval rates for different combinations')\n",
    "    print('4. Find most common AI creator ‚Üí reviewer patterns')\n",
    "\n",
    "else:\n",
    "    print('‚ùå No review tables loaded successfully')\n",
    "    print('\\nüîç Let\\'s check what files are actually available in the dataset:')\n",
    "    \n",
    "    # Try to list available files\n",
    "    try:\n",
    "        from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "        \n",
    "        print('Checking available dataset configurations...')\n",
    "        configs = get_dataset_config_names(\"hao-li/AIDev\")\n",
    "        print(f'Available configs: {configs}')\n",
    "        \n",
    "        splits = get_dataset_split_names(\"hao-li/AIDev\")\n",
    "        print(f'Available splits: {splits}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Could not check dataset structure: {e}')\n",
    "        \n",
    "        print('\\nüìù Alternative: Create sample review data for analysis demonstration')\n",
    "        print('Would you like to proceed with sample data to show the analysis method?')\n",
    "\n",
    "print(f'\\n‚úÖ Review data loading phase complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# COMPLETE AI TOOL COMBINATIONS ANALYSIS - ALL AI-TO-AI COMBINATIONS\n",
    "print('AI CREATOR ‚Üí REVIEWER COMBINATIONS (REAL DATA)')\n",
    "print('=' * 50)\n",
    "\n",
    "# Use the correct column names from the merge\n",
    "reviews_df = review_tables['pr_reviews']\n",
    "merged_data = pull_requests_df.merge(\n",
    "    reviews_df, \n",
    "    left_on='id',       \n",
    "    right_on='pr_id',   \n",
    "    how='inner',\n",
    "    suffixes=('_pr', '_review')\n",
    ")\n",
    "\n",
    "print(f'Data Overview:')\n",
    "print(f'   ‚Ä¢ AI PRs with reviews: {len(merged_data):,}')\n",
    "print(f'   ‚Ä¢ Coverage: {(len(merged_data)/len(pull_requests_df)*100):.1f}% of AI PRs have reviews')\n",
    "\n",
    "# Simple combination counting - just frequency\n",
    "combinations = merged_data.groupby(['agent', 'user_review']).size().reset_index(name='review_count')\n",
    "combinations.columns = ['ai_creator', 'reviewer', 'review_count']\n",
    "combinations = combinations.sort_values('review_count', ascending=False)\n",
    "\n",
    "print(f'\\nFound {len(combinations):,} unique AI creator ‚Üí reviewer combinations')\n",
    "\n",
    "print(f'\\nTOP 15 AI CREATOR ‚Üí REVIEWER COMBINATIONS:')\n",
    "print('-' * 55)\n",
    "print(f'{\"#\":<3} {\"AI Creator\":<15} {\"Reviewer\":<25} {\"Reviews\":<8}')\n",
    "print('-' * 55)\n",
    "for i, (_, row) in enumerate(combinations.head(15).iterrows(), 1):\n",
    "    creator = str(row['ai_creator'])[:14]\n",
    "    reviewer = str(row['reviewer'])[:24]\n",
    "    count = int(row['review_count'])\n",
    "    \n",
    "    print(f'{i:<3} {creator:<15} {reviewer:<25} {count:<8,}')\n",
    "\n",
    "# AI/Bot reviewers identification\n",
    "bot_reviewers = [r for r in combinations['reviewer'].unique() \n",
    "                if isinstance(r, str) and '[bot]' in r.lower()]\n",
    "\n",
    "print(f'\\nAI/BOT REVIEWERS IDENTIFIED: {len(bot_reviewers)}')\n",
    "print('ALL AI/BOT REVIEWERS:')\n",
    "print('-' * 40)\n",
    "for i, bot in enumerate(bot_reviewers, 1):\n",
    "    print(f'  {i:2d}. {bot}')\n",
    "\n",
    "# ALL AI creator ‚Üí AI reviewer combinations\n",
    "ai_to_ai = combinations[combinations['reviewer'].isin(bot_reviewers)]\n",
    "ai_to_ai = ai_to_ai.sort_values('review_count', ascending=False)\n",
    "\n",
    "print(f'\\nü§ñ ALL AI CREATOR ‚Üí AI REVIEWER COMBINATIONS ({len(ai_to_ai)} total):')\n",
    "print('=' * 70)\n",
    "print(f'{\"#\":<4} {\"AI Creator\":<18} {\"AI Reviewer\":<35} {\"Reviews\":<10}')\n",
    "print('=' * 70)\n",
    "\n",
    "# Show ALL AI-to-AI combinations\n",
    "for i, (_, row) in enumerate(ai_to_ai.iterrows(), 1):\n",
    "    creator = str(row['ai_creator'])[:17]\n",
    "    reviewer = str(row['reviewer'])[:34]\n",
    "    count = int(row['review_count'])\n",
    "    \n",
    "    print(f'{i:<4} {creator:<18} {reviewer:<35} {count:<10,}')\n",
    "\n",
    "# Detailed analysis by AI creator\n",
    "print(f'\\nüìä DETAILED BREAKDOWN BY AI CREATOR:')\n",
    "print('=' * 50)\n",
    "\n",
    "for creator in combinations['ai_creator'].unique():\n",
    "    creator_data = ai_to_ai[ai_to_ai['ai_creator'] == creator]\n",
    "    if len(creator_data) > 0:\n",
    "        total_reviews = creator_data['review_count'].sum()\n",
    "        print(f'\\nüîß {creator} ({total_reviews:,} AI reviews):')\n",
    "        print('-' * 45)\n",
    "        \n",
    "        for j, (_, row) in enumerate(creator_data.iterrows(), 1):\n",
    "            reviewer = str(row['reviewer'])\n",
    "            count = int(row['review_count'])\n",
    "            percentage = (count / total_reviews) * 100\n",
    "            print(f'   {j:2d}. {reviewer:<30} {count:>6,} ({percentage:4.1f}%)')\n",
    "\n",
    "# Summary statistics\n",
    "print(f'\\nüìà COMPREHENSIVE SUMMARY:')\n",
    "print('=' * 30)\n",
    "\n",
    "# Reviews by AI creator\n",
    "print(f'\\nREVIEWS BY AI CREATOR TOOL:')\n",
    "print('-' * 30)\n",
    "creator_totals = combinations.groupby('ai_creator')['review_count'].sum().sort_values(ascending=False)\n",
    "for tool, total in creator_totals.items():\n",
    "    ai_reviews = ai_to_ai[ai_to_ai['ai_creator'] == tool]['review_count'].sum()\n",
    "    ai_percentage = (ai_reviews / total) * 100 if total > 0 else 0\n",
    "    print(f'{tool:<18} {total:>8,} total ({ai_reviews:>6,} AI reviews, {ai_percentage:4.1f}%)')\n",
    "\n",
    "# Most active AI reviewers\n",
    "print(f'\\nMOST ACTIVE AI REVIEWERS:')\n",
    "print('-' * 35)\n",
    "ai_reviewer_totals = ai_to_ai.groupby('reviewer')['review_count'].sum().sort_values(ascending=False)\n",
    "for reviewer, total in ai_reviewer_totals.items():\n",
    "    reviewer_short = str(reviewer)[:40]\n",
    "    print(f'{reviewer_short:<40} {total:>8,}')\n",
    "\n",
    "# Cross-platform vs same-platform analysis\n",
    "print(f'\\nüîó ECOSYSTEM ANALYSIS:')\n",
    "print('-' * 25)\n",
    "\n",
    "# Detect same-platform combinations (simplified heuristic)\n",
    "same_platform = 0\n",
    "cross_platform = 0\n",
    "\n",
    "for _, row in ai_to_ai.iterrows():\n",
    "    creator = str(row['ai_creator']).lower()\n",
    "    reviewer = str(row['reviewer']).lower()\n",
    "    count = row['review_count']\n",
    "    \n",
    "    # Simple heuristic for same platform\n",
    "    is_same_platform = False\n",
    "    if 'copilot' in creator and 'copilot' in reviewer:\n",
    "        is_same_platform = True\n",
    "    elif 'cursor' in creator and 'cursor' in reviewer:\n",
    "        is_same_platform = True\n",
    "    elif 'codex' in creator and any(word in reviewer for word in ['openai', 'gpt']):\n",
    "        is_same_platform = True\n",
    "    \n",
    "    if is_same_platform:\n",
    "        same_platform += count\n",
    "    else:\n",
    "        cross_platform += count\n",
    "\n",
    "total_ai_reviews = same_platform + cross_platform\n",
    "if total_ai_reviews > 0:\n",
    "    print(f'Same-platform combinations: {same_platform:,} ({same_platform/total_ai_reviews*100:.1f}%)')\n",
    "    print(f'Cross-platform combinations: {cross_platform:,} ({cross_platform/total_ai_reviews*100:.1f}%)')\n",
    "\n",
    "print(f'\\nüéØ KEY INSIGHTS:')\n",
    "print('-' * 15)\n",
    "print(f'   ‚Ä¢ Total unique combinations: {len(combinations):,}')\n",
    "print(f'   ‚Ä¢ AI ‚Üí AI combinations: {len(ai_to_ai):,}')\n",
    "print(f'   ‚Ä¢ AI ‚Üí Human combinations: {len(combinations) - len(ai_to_ai):,}')\n",
    "print(f'   ‚Ä¢ Most reviewed AI tool: {creator_totals.index[0]}')\n",
    "print(f'   ‚Ä¢ Most active AI reviewer: {ai_reviewer_totals.index[0]}')\n",
    "print(f'   ‚Ä¢ Total AI creators: {len(ai_to_ai[\"ai_creator\"].unique())}')\n",
    "print(f'   ‚Ä¢ Total AI reviewers: {len(ai_to_ai[\"reviewer\"].unique())}')\n",
    "\n",
    "# Store for potential visualization\n",
    "combinations_for_viz = combinations\n",
    "ai_combinations_complete = ai_to_ai\n",
    "\n",
    "print(f'\\n‚úÖ COMPLETE AI COMBINATION ANALYSIS FINISHED!')\n",
    "print(f'üìä Variables created: combinations_for_viz, ai_combinations_complete')\n",
    "print(f'üöÄ Ready for visualization and further analysis!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FOCUSED HEATMAP: TOP REVIEWERS ONLY\n",
    "print(\"Creating focused heatmaps with top reviewers...\")\n",
    "\n",
    "# 1. Filter to top 20 most active reviewers\n",
    "top_reviewers = ai_combinations_complete.groupby('reviewer')['review_count'].sum().nlargest(20).index\n",
    "ai_top_reviewers = ai_combinations_complete[ai_combinations_complete['reviewer'].isin(top_reviewers)]\n",
    "\n",
    "# Create pivot for top reviewers only\n",
    "pivot_focused = ai_top_reviewers.pivot(\n",
    "    index='ai_creator', \n",
    "    columns='reviewer', \n",
    "    values='review_count'\n",
    ").fillna(0)\n",
    "\n",
    "print(f\"Focused heatmap: {pivot_focused.shape[0]} creators √ó {pivot_focused.shape[1]} reviewers\")\n",
    "\n",
    "# 2. REGULAR HEATMAP - TOP 20 REVIEWERS\n",
    "plt.figure(figsize=(18, 8))\n",
    "sns.heatmap(pivot_focused, \n",
    "            annot=True,\n",
    "            fmt='.0f',\n",
    "            cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Review Count'},\n",
    "            linewidths=0.5)\n",
    "\n",
    "plt.title('AI Creator ‚Üí AI Reviewer Combinations (Top 20 Most Active Reviewers)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('AI Reviewers', fontsize=12)\n",
    "plt.ylabel('AI Creators', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. LOG-SCALE HEATMAP - TOP 20 REVIEWERS\n",
    "plt.figure(figsize=(18, 8))\n",
    "pivot_focused_log = np.log1p(pivot_focused)\n",
    "\n",
    "sns.heatmap(pivot_focused_log, \n",
    "            annot=True, \n",
    "            fmt='.1f',\n",
    "            cmap='viridis',\n",
    "            cbar_kws={'label': 'Log(1 + Review Count)'},\n",
    "            linewidths=0.5)\n",
    "\n",
    "plt.title('AI Creator ‚Üí AI Reviewer Combinations - Log Scale (Top 20 Reviewers)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('AI Reviewers', fontsize=12)\n",
    "plt.ylabel('AI Creators', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. ALTERNATIVE: Top 10 for even cleaner view\n",
    "top_10_reviewers = ai_combinations_complete.groupby('reviewer')['review_count'].sum().nlargest(10).index\n",
    "ai_top_10 = ai_combinations_complete[ai_combinations_complete['reviewer'].isin(top_10_reviewers)]\n",
    "\n",
    "pivot_top10 = ai_top_10.pivot(\n",
    "    index='ai_creator', \n",
    "    columns='reviewer', \n",
    "    values='review_count'\n",
    ").fillna(0)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(pivot_top10, \n",
    "            annot=True,\n",
    "            fmt='.0f',\n",
    "            cmap='RdYlBu_r',\n",
    "            cbar_kws={'label': 'Review Count'},\n",
    "            linewidths=0.8)\n",
    "\n",
    "plt.title('AI Creator ‚Üí AI Reviewer Combinations (Top 10 Most Active Reviewers)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('AI Reviewers', fontsize=12)\n",
    "plt.ylabel('AI Creators', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Show which reviewers were selected\n",
    "print(f\"\\nTop 20 Reviewers Selected:\")\n",
    "for i, reviewer in enumerate(top_reviewers, 1):\n",
    "    total_reviews = ai_combinations_complete[ai_combinations_complete['reviewer'] == reviewer]['review_count'].sum()\n",
    "    print(f\"{i:2d}. {reviewer:<35} {total_reviews:>6,} reviews\")\n",
    "\n",
    "print(f\"\\nTop 10 Reviewers Selected:\")\n",
    "for i, reviewer in enumerate(top_10_reviewers, 1):\n",
    "    total_reviews = ai_combinations_complete[ai_combinations_complete['reviewer'] == reviewer]['review_count'].sum()\n",
    "    print(f\"{i:2d}. {reviewer:<35} {total_reviews:>6,} reviews\")\n",
    "\n",
    "# 6. Coverage analysis\n",
    "total_reviews_all = ai_combinations_complete['review_count'].sum()\n",
    "total_reviews_top20 = ai_top_reviewers['review_count'].sum()\n",
    "total_reviews_top10 = ai_top_10['review_count'].sum()\n",
    "\n",
    "print(f\"\\nCoverage Analysis:\")\n",
    "print(f\"All 41 reviewers:  {total_reviews_all:,} reviews (100.0%)\")\n",
    "print(f\"Top 20 reviewers:  {total_reviews_top20:,} reviews ({total_reviews_top20/total_reviews_all*100:.1f}%)\")\n",
    "print(f\"Top 10 reviewers:  {total_reviews_top10:,} reviews ({total_reviews_top10/total_reviews_all*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
